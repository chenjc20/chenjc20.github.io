---
title: D2FQ:Device-Direct Fair Queueing for NVMe SSDs
tags: 
  - tag: NVMe
categories: fast
author: 吴旗
---

在云场景下多租户独立I/O流存取，只涉及单盘的公平性调度。

## 以往的工作/背景

### 启发本文来源

- 以往工作hotstorage17使NVM.e在Lnixu块层支持WRR,这个启发了本文在SSD上做一个资源公平调度.并且之前的工作都没有考虑与多个流共享队列，当流的数量超过队列的数量时，这是必要的。
  - **公平性**指的是资源不够的情况下避免存在饿死，同时要考虑资源利用，比如只保证了公平性可能会导致资源浪费,主要体现就是按加权分配
- 从MQFQ最优秀的公平调度算法
- 受到网络分组调度算法的启发,把分组调度常用的offload方法应用到了从cpu到设备层.启发的来源是网络设备经常有设备测的调度功能,而NVM.e同样拥有设备侧的调度如WRR
- [Jinkyu Jeong](http://csl.skku.edu/People/Jinkyu)是成均馆大学副教授,之前做过一篇linux 堆栈io block层调度的工作[atc 19Asynchronous I/O Stack: A Low-latency Kernel I/O Stack for Ultra-Low Latency SSDs]
  - atc19的文章是低延迟异步堆栈io已经做了提交=分配,而没有做调度,和本文结合可以释放更多的cpu占用率

### 背景

> NVM.e SDD支持多核多队列，使linux 的block层架构为了使用新的设备而升级，早在 3.13 内核就已经加入了多队列代码，但是还不太稳定，经过多年的发展 multi-queue 越来越稳定，[linux](https://so.csdn.net/so/search?from=pc_blog_highlight&q=linux) 5.0+ 已经默认使用 multi-queue

## 技术点

设每个工作流的请求长度为l,工作流的权重为w,则虚拟时间是l/w,公平性指的是每个工作流的虚拟时间相同,而实际不可能,所以只能通过调度让其尽可能相同即减少gap.

为了最大化ssd的性能,公平性也可少量不公平,trade-off后发现短暂的不公平换来了最大化利用ssd的性能达到更高吞吐量[11]

![avatar](\assets\img\papers\2021-12-14-D2FQ\image-20211231104711174.png)

- 要做到高效调节队列速度，光有静态H/L是不够的
- 设立合适的阈值，代表了所能容忍的短暂不公平度
- 如何最小资源并有效的track最小虚拟时间

三个问题分别对应三个技术点

### 动态调节H/L

关于H/L，可以理解为抽象后的高速队列和低速队列的理想处理速度比值

![avatar](\assets\img\papers\2021-12-14-D2FQ\image-20211231105415481.png)

结论是提出了两个调节公式，没有推导过程，因为最后做实验是直接吧H/L调到最高为256开始做实验，所以主要是减少H/L的公式比较重要

### 决定阈值

![avatar](\assets\img\papers\2021-12-14-D2FQ\image-20211231105602162.png)

从经验主义来选择最适合实验负载的阈值，最后选择在权重为8的流下τL的值为8MB，其余两个阈值做2的乘除直接得到，这也没有推导过程。

### Sloppy 跟踪策略

只有伪代码，建议直接看，并不复杂。

![avatar](\assets\img\papers\2021-12-14-D2FQ\image-20211231105928161.png)

## 实验

![avatar](\assets\img\papers\2021-12-14-D2FQ\image-20211231110103629.png)

实验部分在实际负载（YCSB）中效果不是很好，在造的负载下对比最好的公平性调度策略MQFQ多释放了45%的CPU利用率



**水平有限，如要了解更多请关注: **[请参考原文](https://www.usenix.org/conference/fast21/presentation/woo)

